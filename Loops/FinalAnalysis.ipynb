{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LoopsResultsAnalysis**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Importing and First Proccesing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from DataCleaning import *\n",
    "from ProcessingConfig import *\n",
    "\n",
    "pd.options.display.max_columns = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original shape: (13294, 32)\n",
      "threshold for outliers detection: 2.25\n",
      "filter_slow_subjects: No slow subjects detected.\n",
      "filter_bad_subjects: No bad subjects detected (in terms of low success rate).\n",
      "drop_first_loop: 1630 rows were filtered out.\n",
      "only_first_line: 9227 rows were filtered out.\n",
      "filter_bad_trials: No bad trials detected (in terms of low success rate).\n",
      "filter_slow_steps: 77 slow steps were filtered out.\n",
      "Here is a summary of slow steps rate per subjects: \n",
      "              slow steps rate (%)\n",
      "subject                     \n",
      "104A                    0.64\n",
      "102A                    1.96\n",
      "106A                    1.96\n",
      "106B                    1.96\n",
      "103A                    2.38\n",
      "102B                    2.58\n",
      "101B                    3.29\n",
      "105A                    3.42\n",
      "109B                    3.87\n",
      "108A                    3.90\n",
      "107B                    4.00\n",
      "104B                    5.11\n",
      "108B                    5.48\n",
      "109A                    5.48\n",
      "103B                    7.09\n",
      "final shape: (2360, 25)\n"
     ]
    }
   ],
   "source": [
    "raw_data = pd.read_excel(cleaning_config['raw_data_path'])\n",
    "print(f'original shape: {raw_data.shape}')\n",
    "outliers_threshold = cleaning_config['filter_threshold']\n",
    "print(f\"threshold for outliers detection: {outliers_threshold}\")\n",
    "\n",
    "drop_columns(raw_data, cleaning_config['unnecessary_columns'])\n",
    "convert_types(raw_data, cleaning_config['type_conversions'])\n",
    "raw_data = filter_slow_subjects(raw_data, outliers_threshold)\n",
    "raw_data = filter_bad_subjects(raw_data, outliers_threshold)\n",
    "raw_data = drop_first_loop(raw_data)\n",
    "raw_data = only_first_line(raw_data)\n",
    "raw_data = filter_bad_trials(raw_data, threshold=0.9)\n",
    "raw_data = filter_slow_steps(raw_data, outliers_threshold)\n",
    "\n",
    "path = cleaning_config['results_path'] + f'_{dt.now().strftime(\"%d.%m.%Y_%H-%M\")}.xlsx'\n",
    "raw_data.to_excel(path)\n",
    "\n",
    "print(f'final shape: {raw_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of subjects: 15\n",
      "number of sessions: 29\n"
     ]
    }
   ],
   "source": [
    "n_subjects = raw_data['subject'].nunique()\n",
    "print(f'number of subjects: {n_subjects}')\n",
    "\n",
    "n_sessions = raw_data[['subject', 'trial_set']].drop_duplicates().shape[0]\n",
    "print(f'number of sessions: {n_sessions}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Priming Effect Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "\n",
    "# enveloping t_test and f_test functions\n",
    "\n",
    "def f_test(smp1, smp2, alpha):\n",
    "    f_score = smp1.std() / smp2.std()\n",
    "    df1, df2 = smp1.size - 1, smp2.size - 1\n",
    "    return stats.f.cdf(f_score, df1, df2) >= alpha\n",
    "\n",
    "def t_test_ind(smp1, smp2, alpha, alternative='two-sided'):\n",
    "    equal_var = f_test(smp1, smp2, alpha)\n",
    "    t_score, p_value = stats.ttest_ind(smp1, smp2, alternative=alternative, equal_var=equal_var)\n",
    "    \n",
    "    print(f'p_value: {p_value}')\n",
    "    if p_value <= alpha:\n",
    "        print(f'There is a significant difference between the samples! ({(1-alpha)*100}%).')\n",
    "    else:\n",
    "        print(f'It is not possible to determine whether there is an effect ({(1-alpha)*100}%).')\n",
    "    return t_score, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean_response_time</th>\n",
       "      <th>mean_success_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loop_type</th>\n",
       "      <th>loop_type_switch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">for</th>\n",
       "      <th>False</th>\n",
       "      <td>3254.087179</td>\n",
       "      <td>0.976628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>3249.750433</td>\n",
       "      <td>0.976311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">while</th>\n",
       "      <th>False</th>\n",
       "      <td>3536.781362</td>\n",
       "      <td>0.953846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>3670.382979</td>\n",
       "      <td>0.964103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            mean_response_time  mean_success_rate\n",
       "loop_type loop_type_switch                                       \n",
       "for       False                    3254.087179           0.976628\n",
       "          True                     3249.750433           0.976311\n",
       "while     False                    3536.781362           0.953846\n",
       "          True                     3670.382979           0.964103"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean response time and success rate, group by loop type and loop switching.\n",
    "switching_diff = pd.DataFrame()\n",
    "\n",
    "switching_diff['mean_response_time'] = raw_data[raw_data['correct']].groupby(['loop_type', 'loop_type_switch'])['rt'].mean()\n",
    "switching_diff['mean_success_rate'] = raw_data.groupby(['loop_type', 'loop_type_switch'])['correct'].mean()\n",
    "\n",
    "switching_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_value: 0.5169306161550555\n",
      "It is not possible to determine whether there is an effect (95.0%).\n"
     ]
    }
   ],
   "source": [
    "# checking for priming effect on 'for' loops\n",
    "same = raw_data.loc[raw_data['correct'] & ( ~ raw_data['loop_type_switch']) & (raw_data['loop_type'] == 'for'), 'rt']\n",
    "different = raw_data.loc[raw_data['correct'] & (raw_data['loop_type_switch']) & (raw_data['loop_type'] == 'for'), 'rt']\n",
    "\n",
    "t_score, p_value = t_test_ind(different, same, alpha=alpha, alternative='greater')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_value: 0.12093149698869998\n",
      "It is not possible to determine whether there is an effect (95.0%).\n"
     ]
    }
   ],
   "source": [
    "# checking for priming effect on 'while' loops\n",
    "same = raw_data.loc[raw_data['correct'] & ( ~ raw_data['loop_type_switch']) & (raw_data['loop_type'] == 'while'), 'rt']\n",
    "different = raw_data.loc[raw_data['correct'] & (raw_data['loop_type_switch']) & (raw_data['loop_type'] == 'while'), 'rt']\n",
    "\n",
    "t_score, p_value = t_test_ind(different, same, alpha=alpha, alternative='greater')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
