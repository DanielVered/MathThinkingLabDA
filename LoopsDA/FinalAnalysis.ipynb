{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LoopsResultsAnalysis**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Importing and First Proccesing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import plotly.express as px\n",
    "from DataCleaning import *\n",
    "from ProcessingConfig import *\n",
    "from datetime import datetime as dt\n",
    "\n",
    "pd.options.display.max_columns = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original shape: (17408, 32)\n",
      "threshold for outliers detection: 2.25\n",
      "filter_slow_subjects: No slow subjects detected.\n",
      "filter_bad_subjects: No bad subjects detected (in terms of low success rate).\n",
      "drop_first_loop: 2134 rows were filtered out.\n",
      "only_first_line: 12081 rows were filtered out.\n",
      "filter_bad_trials: No bad trials detected (in terms of low success rate).\n",
      "filter_slow_steps: 87 slow steps were filtered out.\n",
      "Here is a summary of slow steps rate per subjects: \n",
      "              slow steps rate (%)\n",
      "subject                     \n",
      "101A                    0.00\n",
      "110A                    0.00\n",
      "110B                    0.00\n",
      "104A                    0.64\n",
      "111A                    0.64\n",
      "102A                    1.96\n",
      "106B                    1.96\n",
      "106A                    1.96\n",
      "103A                    2.38\n",
      "102B                    2.58\n",
      "101B                    3.29\n",
      "105A                    3.42\n",
      "107A                    3.57\n",
      "109B                    3.87\n",
      "108A                    3.90\n",
      "107B                    4.00\n",
      "105B                    4.76\n",
      "104B                    5.11\n",
      "108B                    5.48\n",
      "109A                    5.48\n",
      "103B                    7.09\n",
      "final shape: (3106, 25)\n"
     ]
    }
   ],
   "source": [
    "raw_data = pd.read_excel(cleaning_config['raw_data_path'])\n",
    "print(f'original shape: {raw_data.shape}')\n",
    "outliers_threshold = cleaning_config['filter_threshold']\n",
    "print(f\"threshold for outliers detection: {outliers_threshold}\")\n",
    "\n",
    "drop_columns(raw_data, cleaning_config['unnecessary_columns'])\n",
    "convert_types(raw_data, cleaning_config['type_conversions'])\n",
    "raw_data = filter_slow_subjects(raw_data, outliers_threshold)\n",
    "raw_data = filter_bad_subjects(raw_data, outliers_threshold)\n",
    "raw_data = drop_first_loop(raw_data)\n",
    "raw_data = only_first_line(raw_data)\n",
    "raw_data = filter_bad_trials(raw_data, threshold=analysis_config['trials_success_rate_threshold'])\n",
    "raw_data = filter_slow_steps(raw_data, outliers_threshold)\n",
    "\n",
    "path = cleaning_config['results_path'] + f'_{dt.now().strftime(\"%d.%m.%Y_%H-%M\")}.xlsx'\n",
    "raw_data.to_excel(path)\n",
    "\n",
    "print(f'final shape: {raw_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of subjects: 21\n",
      "number of sessions: 38\n"
     ]
    }
   ],
   "source": [
    "n_subjects = raw_data['subject'].nunique()\n",
    "print(f'number of subjects: {n_subjects}')\n",
    "\n",
    "n_sessions = raw_data[['subject', 'trial_set']].drop_duplicates().shape[0]\n",
    "print(f'number of sessions: {n_sessions}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Priming Effect Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "\n",
    "# enveloping t_test and f_test functions\n",
    "\n",
    "def f_test(smp1, smp2, alpha):\n",
    "    f_score = smp1.std() / smp2.std()\n",
    "    df1, df2 = smp1.size - 1, smp2.size - 1\n",
    "    return stats.f.cdf(f_score, df1, df2) >= alpha\n",
    "\n",
    "def t_test_ind(smp1, smp2, alpha, alternative='two-sided'):\n",
    "    equal_var = f_test(smp1, smp2, alpha)\n",
    "    t_score, p_value = stats.ttest_ind(smp1, smp2, alternative=alternative, equal_var=equal_var)\n",
    "    \n",
    "    print(f'p_value: {p_value}')\n",
    "    if p_value <= alpha:\n",
    "        print(f'There is a significant difference between the samples! ({(1-alpha)*100}%).')\n",
    "    else:\n",
    "        print(f'It is not possible to determine whether there is an effect ({(1-alpha)*100}%).')\n",
    "    return t_score, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean_response_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loop_type</th>\n",
       "      <th>loop_type_switch</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">for</th>\n",
       "      <th>False</th>\n",
       "      <td>3290.793194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>3301.628989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">while</th>\n",
       "      <th>False</th>\n",
       "      <td>3580.155827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>3652.074224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            mean_response_time\n",
       "loop_type loop_type_switch                    \n",
       "for       False                    3290.793194\n",
       "          True                     3301.628989\n",
       "while     False                    3580.155827\n",
       "          True                     3652.074224"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean response time and success rate, group by loop type and loop switching.\n",
    "switching_diff_basic = pd.DataFrame()\n",
    "\n",
    "switching_diff_basic['mean_response_time'] = raw_data[raw_data['correct']].groupby(['loop_type', 'loop_type_switch'])['rt'].mean()\n",
    "\n",
    "switching_diff_basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation between loop type switching and response time is not significant (p = 0.259), with a value of r = 0.02\n"
     ]
    }
   ],
   "source": [
    "# checking for a correlation between switching and response time\n",
    "r, p_val = stats.pearsonr(raw_data['loop_type_switch'], raw_data['rt'])\n",
    "significance = 'significant' if p_val < 0.05 else 'not significant'\n",
    "print(f'Pearson correlation between loop type switching and response time is {significance} (p = {round(p_val, 3)}), with a value of r = {round(r, 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_value: 0.4507441623576721\n",
      "It is not possible to determine whether there is an effect (95.0%).\n"
     ]
    }
   ],
   "source": [
    "# checking for priming effect on 'for' loops\n",
    "same = raw_data.loc[raw_data['correct'] & ( ~ raw_data['loop_type_switch']) & (raw_data['loop_type'] == 'for'), 'rt']\n",
    "different = raw_data.loc[raw_data['correct'] & (raw_data['loop_type_switch']) & (raw_data['loop_type'] == 'for'), 'rt']\n",
    "\n",
    "t_score, p_value = t_test_ind(different, same, alpha=alpha, alternative='greater')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_value: 0.2272831032848695\n",
      "It is not possible to determine whether there is an effect (95.0%).\n"
     ]
    }
   ],
   "source": [
    "# checking for priming effect on 'while' loops\n",
    "same = raw_data.loc[raw_data['correct'] & ( ~ raw_data['loop_type_switch']) & (raw_data['loop_type'] == 'while'), 'rt']\n",
    "different = raw_data.loc[raw_data['correct'] & (raw_data['loop_type_switch']) & (raw_data['loop_type'] == 'while'), 'rt']\n",
    "\n",
    "t_score, p_value = t_test_ind(different, same, alpha=alpha, alternative='greater')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
